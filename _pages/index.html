---
layout: archive
title: ""
permalink: /publications/
author_profile: true
---

<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Keling Yao</title>
  <meta name="author" content="Kenny Yao">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="icon" href="../images/favicon/favicon.ico" type="image/x-icon">
  <link rel="stylesheet" href="stylesheet.css">
</head>
<body>
  <table style="width:100%; border:0; border-collapse:collapse; margin:auto;">
    <tr>
      <td>
        <table style="width:100%; border:0; border-collapse:collapse; margin:auto;">
          <tr>
            <td style="padding:20px;">
              <h2>Research</h2>
              <p>
                I'm interested in computer vision, deep learning, generative AI, and robot learning.
              </p>
            </td>
          </tr>
        </table>
        <table style="width:100%; border:0; border-collapse:collapse; margin:auto;">
          <tr style="background-color:#ffffff;">
            <td style="padding:20px;">
              <div class="one">
                <div class="two">
                  <img src="../images/MSRA.png" width="160">
                </div>
              </div>
            </td>
            <td style="padding:20px;">
              <a href="https://arxiv.org/abs/2306.05716">
                <span class="papertitle">Transferring Foundation Models for Generalizable Robotic Manipulation</span>
              </a>
              <br>
              Jiange Yang, Wenhui Tan, Chuhao Jin, <strong>Keling Yao</strong>, Bei Liu, Jianlong Fu, Ruihua Song, Gangshan Wu, Limin Wang
              <br>
              <em>arXiv</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2306.05716">arXiv</a> /
              <a href="https://www.youtube.com/watch?v=1m9wNzfp_4E&t=1s">Video</a>
              <p>
                We propose a novel paradigm that effectively leverages language-reasoning segmentation mask generated by internet-scale foundation models, to condition robot manipulation tasks.
              </p>
            </td>
          </tr>
          <tr style="background-color:#ffffff;">
            <td style="padding:20px;">
              <div class="one">
                <div class="two">
                  <img src="../images/dttd2.png" width="160">
                </div>
              </div>
            </td>
            <td style="padding:20px;">
              <a href="https://github.com/augcog/DTTD2">
                <span class="papertitle">Robust Digital-Twin Localization via An RGBD-based Transformer Network and A Comprehensive Evaluation on a Mobile Dataset</span>
              </a>
              <br>
              Zixun Huang*, <strong>Keling Yao*</strong>, Seth Z. Zhao*, Chuanyu Pan*, Tianjian Xu, Weiyu Feng, Allen Y. Yang
              <br>
              <em>arXiv</em>, 2024
              <br>
              <a href="https://github.com/augcog/DTTD2">project page</a> /
              <a href="https://arxiv.org/abs/2309.13570">arXiv</a> /
              <a href="https://youtu.be/QhYWyoPTmOk">Video</a>
              <p>
                We propose a transformer-based 6DoF pose estimator designed to achieve state-of-the-art accuracy under real-world noisy data. To systematically validate the new solution's performance against the prior art, we also introduce a novel RGBD dataset called Digital Twin Tracking Dataset v2 (DTTD2).
              </p>
            </td>
          </tr>
        </table>
      </td>
    </tr>
  </table>
</body>
</html>
